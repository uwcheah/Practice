{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('research': conda)",
   "display_name": "Python 3.8.5 64-bit ('research': conda)",
   "metadata": {
    "interpreter": {
     "hash": "a9b812a2d14064c1d26f3cde9a42b810c9d8274b97c3f96915e458059254eb00"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Analysis of S&P 500 returns for Alliance Bernstein\n",
    "## Author: Ui-Wing Cheah"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Import statements"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import scipy as sp \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import ab_util as util \n",
    "from datetime import datetime,timedelta\n"
   ]
  },
  {
   "source": [
    "## Processing the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in the raw data\n",
    "raw_data = pd.read_excel('spx_data.xlsx',index_col=0)\n",
    "os_start = datetime(2009,12,31)\n",
    "# we want to process the data into the following columns\n",
    "# dates,prices,returns,year,tri,quint,dec,sample (0/1)\n",
    "# keep the data in numerical format as much as possible\n",
    "\n",
    "all_dts = raw_data.index.to_pydatetime()\n",
    "data_df = raw_data\n",
    "\n",
    "# computing returns\n",
    "data_df['returns'] = data_df['prices'].divide(data_df['prices'].shift(1)).subtract(1)\n",
    "# assigning year\n",
    "data_df['year'] = pd.Series(index=all_dts,data=np.array([dt.year for dt in all_dts]))\n",
    "data_df['sample'] = pd.Series(index=all_dts,data=0)\n",
    "data_df['sample'][data_df.index>os_start]=1\n",
    "data_df['daycount'] = pd.Series(index=all_dts,data=1.).cumsum()\n",
    "# we drop the first date for convenience since there's no return\n",
    "data_df = data_df.dropna(how='any',axis=0)\n",
    "\n",
    "# we now take out the unique years\n",
    "all_years = sorted(set(data_df['year'].values))\n",
    "\n",
    "# we now break out the decades\n",
    "dec_interval = all_years[9::10]\n",
    "quint_interval = all_years[4::5]\n",
    "tri_interval = all_years[2::3]\n",
    "\n",
    "# label the decades\n",
    "y0 = 1928\n",
    "dec_series = pd.Series()\n",
    "for iy,y in enumerate(dec_interval):\n",
    "    ds_ = pd.Series(index = data_df[data_df['year']<=y][data_df['year']>y0].index,data=iy+1)\n",
    "    dec_series = dec_series.combine_first(ds_)\n",
    "    y0 =y\n",
    "data_df['dec'] = dec_series\n",
    "\n",
    "# label the 5-year points\n",
    "y0 = 1928\n",
    "quint_series = pd.Series()\n",
    "for iy,y in enumerate(quint_interval):\n",
    "    ds_ = pd.Series(index = data_df[data_df['year']<=y][data_df['year']>y0].index,data=iy+1)\n",
    "    quint_series = quint_series.combine_first(ds_)\n",
    "    y0 =y\n",
    "data_df['quint'] = quint_series\n",
    "\n",
    "# label the 3-year points\n",
    "y0 = 1928\n",
    "tri_series = pd.Series()\n",
    "for iy,y in enumerate(tri_interval):\n",
    "    ds_ = pd.Series(index = data_df[data_df['year']<=y][data_df['year']>y0].index,data=iy+1)\n",
    "    tri_series = tri_series.combine_first(ds_)\n",
    "    y0 =y\n",
    "data_df['tri'] = tri_series\n"
   ]
  },
  {
   "source": [
    "## Part 1: Comparing return distributions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will focus on the just the 3Y samples \n",
    "tri_samples = sorted(set(data_df['tri'].values))\n",
    "os_samples = [28,29,30]\n",
    "sample_names = {28:'10-12',29:'13-15',30:'16-18'}\n",
    "\n",
    "# getting the summary statistics of the 3Y data\n",
    "sample_data_3Y = pd.DataFrame({i:util.get_sample_stats(data_df[data_df['tri']==i]['returns']) for i in tri_samples}).T\n",
    "\n",
    "\n",
    "# computing the p-values of the sample averages and other statistic\n",
    "sample_pval = pd.DataFrame({i:util.estimate_pvalue(sample_data_3Y.loc[i],sample_data_3Y) for i in os_samples}).rename(columns=sample_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the central chart \n",
    "fig_ = plt.figure(figsize=(12,8))\n",
    "ax_test = util.box_scatter(sample_data_3Y,os_samples=os_samples,plot_cols='center',scatter_names = sample_names,legend_dict={'loc':4},xlabel='Measure',ylabel='Daily Return')\n",
    "ax_test.set_title('Fig 1: Central Returns',fontdict={'size':20})\n",
    "fig_.savefig('charts\\p1_ret_cent.png')\n",
    "\n",
    "# plotting the dispersion chart \n",
    "fig_ = plt.figure(figsize=(12,8))\n",
    "ax_test = util.box_scatter(sample_data_3Y,os_samples=os_samples,plot_cols='disp',scatter_names = sample_names,legend_dict={'loc':1},xlabel='Dispersion Measure',ylabel='Daily Return')\n",
    "ax_test.set_title('Fig 2: Return Dispersion',fontdict={'size':20})\n",
    "fig_.savefig('charts\\p2_ret_disp.png')\n",
    "\n",
    "# plotting the chart of tail returns\n",
    "fig_ = plt.figure(figsize=(12,8))\n",
    "ax_test = util.box_scatter(sample_data_3Y,os_samples=os_samples,scatter_names = sample_names,legend_dict={'loc':4},xlabel='Distribution Tail',ylabel='Daily Return')\n",
    "ax_test.set_title('Fig 3: Tail Return Averages',fontdict={'size':20})\n",
    "fig_.savefig('charts\\p3_ret_tail.png')\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Part 2: Volatility Levels and Changes\n",
    "Here we compute the scores of returns based on rolling estimates of volatility"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rets = data_df['returns']\n",
    "min_obs = 63\n",
    "\n",
    "vol_estimates = pd.DataFrame({'sigma_EW':rets.expanding(min_periods=min_obs,).std(),\n",
    "'sigma_RW':rets.rolling(min_periods=min_obs,window=252).std(),\n",
    "'sigma_HL2W':rets.ewm(halflife=10,min_periods=min_obs).std(),\n",
    "'sigma_HL1M':rets.ewm(halflife=21,min_periods=min_obs).std(),\n",
    "'sigma_HL3M':rets.ewm(halflife=63,min_periods=min_obs).std(),\n",
    "'sigma_HL6M': rets.ewm(halflife=126,min_periods=min_obs).std()})\n",
    "\n",
    "# compute square returns\n",
    "vol_estimates['sqret'] = rets.pow(2)\n",
    "\n",
    "# estimating volatilities\n",
    "vol_changes = vol_estimates.subtract(vol_estimates.shift(1))\n",
    "vol_changes = vol_changes.rename(columns={x:'volchg_{0}'.format(x.split('_')[-1]) for x in vol_estimates})\n",
    "\n",
    "# computing vol adjustd returns\n",
    "ret_scores = pd.DataFrame({'score_{0}'.format(x.split('_')[-1]):rets.divide(vol_estimates[x].shift(1)) for x in vol_estimates})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the distribution of vol changes\n",
    "# we will focus on 2W HL but we can always change this\n",
    "chgvar = 'volchg_HL2W'\n",
    "\n",
    "vol_changes = data_df.combine_first(vol_changes)\n",
    "volch_data = pd.DataFrame({i:util.get_sample_stats(vol_changes[vol_changes['tri']==i][chgvar]) for i in tri_samples}).T\n",
    "volch_pval = pd.DataFrame({i:util.estimate_pvalue(volch_data.loc[i],volch_data) for i in os_samples}).rename(columns=sample_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting volatility forecasts and levels date\n",
    "# vestimator = 'sigma_HL1M'\n",
    "vestimator = 'sigma_HL2W'\n",
    "vol_df = data_df.combine_first(vol_estimates)\n",
    "vol_estim_samples = pd.DataFrame({i:util.get_sample_stats(vol_df[data_df['tri']==i][vestimator]) for i in tri_samples}).T\n",
    "vol_sample_pval = pd.DataFrame({i:util.estimate_pvalue(vol_estim_samples.loc[i],vol_estim_samples) for i in os_samples}).rename(columns=sample_names)\n",
    "\n",
    "# Plotting vol levels\n",
    "# plotting the central chart \n",
    "fig_ = plt.figure(figsize=(12,8))\n",
    "ax_test = util.box_scatter(vol_estim_samples,os_samples=os_samples,plot_cols='center',scatter_names=sample_names,legend_dict={'loc':2},xlabel='Measure of Volatility Estimates',ylabel='Daily Volatility')\n",
    "ax_test.set_title('Fig 4: Central Volatility',fontdict={'size':20})\n",
    "fig_.savefig('charts\\p4_vol_cent.png')\n",
    "\n",
    "# plotting the tails of volatility estimates\n",
    "fig_ = plt.figure(figsize=(12,8))\n",
    "ax_test = util.box_scatter(vol_estim_samples,os_samples=os_samples,scatter_names = sample_names,legend_dict={'loc':2},xlabel='Distribution Tail',ylabel='Daily Volatility')\n",
    "ax_test.set_title('Fig 6: Tail Volatility',fontdict={'size':20}, )\n",
    "fig_.savefig('charts\\p6_vol_tail.png')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting volatility forecasts and levels date\n",
    "vchestimator = 'sigma_HL2W'\n",
    "volch_df = data_df.combine_first(vol_changes)\n",
    "volch_estim_samples = pd.DataFrame({i:util.get_sample_stats(volch_df[volch_df['tri']==i]['volchg_HL1M']) for i in tri_samples}).T\n",
    "volch_sample_pval = pd.DataFrame({i:util.estimate_pvalue(volch_estim_samples.loc[i],volch_estim_samples) for i in os_samples}).rename(columns=sample_names)\n",
    "\n",
    "# Plotting vol changes\n",
    "# plotting the central chart \n",
    "fig_ = plt.figure(figsize=(12,8))\n",
    "ax_test = util.box_scatter(volch_estim_samples,os_samples=os_samples,plot_cols='center',scatter_names=sample_names,legend_dict={'loc':3},xlabel='Measure of Volatility Changes',ylabel='Daily Volatility Change')\n",
    "ax_test.set_title('Fig 5: Central Volatility Changes',fontdict={'size':20},)\n",
    "fig_.savefig('charts\\p5_volch_cent.png')\n",
    "\n",
    "# plotting the tails of volatility changes\n",
    "fig_ = plt.figure(figsize=(12,8))\n",
    "ax_test = util.box_scatter(volch_estim_samples,os_samples=os_samples,scatter_names = sample_names,legend_dict={'loc':2},xlabel='Distribution Tail',ylabel='Daily Volatility Change')\n",
    "ax_test.set_title('Fig 7: Tail Volatility Changes',fontdict={'size':20},)\n",
    "fig_.savefig('charts\\p7_volch_tail.png')\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "## PART 3: VOLATILITY SURPRISES"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimating vol thresholds\n",
    "# surprises are labeled as extremely negative (nnn), very negative (nn) , ...extremely positive (ppp)\n",
    "# define the thresholds as per normal distribution\n",
    "# use 1M halflife for this analysis\n",
    "thresholds = [-2.33,-1.96,-1.65,1.65,1.96,2.33]\n",
    "threshnames = {-2.33:'s_nnn',-1.96:'s_nn',-1.65:'s_n',1.65:'s_p',1.96:'s_pp',2.33:'s_ppp'}\n",
    "vol_estim = ['HL1M',]\n",
    "# defining some simple helper functions\n",
    "def calc_th(x,th,dropfalse=False):\n",
    "    if dropfalse:\n",
    "        if th < 0:\n",
    "            y = x[x<=th]\n",
    "        else:\n",
    "            y = x[x>=th]\n",
    "        y = y*0+1\n",
    "        return y\n",
    "\n",
    "    else:\n",
    "        y = x*0\n",
    "        if th < 0:\n",
    "            y[x<=th]=1\n",
    "        else:\n",
    "            y[x>=th]=1\n",
    "        return y\n",
    "\n",
    "# this computes the percentage of surprises relative to total sample size\n",
    "def pctg_surprises(sdf,):\n",
    "    pctg = {}\n",
    "    for i in tri_samples:\n",
    "        sdf_ = sdf[sdf['tri']==i].reindex(columns=['s_nnn','s_nn','s_n','s_p','s_pp','s_ppp'])\n",
    "        pctg[i] = sdf_.sum().divide(len(sdf_.index))\n",
    "    return pd.DataFrame(pctg).T\n",
    "\n",
    "def med_surp_intervals(sdf,):\n",
    "    surptime = {}\n",
    "    cols = ['s_nnn','s_nn','s_n','s_p','s_pp','s_ppp']\n",
    "    for j in cols:\n",
    "        sdf_ = (sdf[j]*sdf['daycount']).dropna()\n",
    "        surptime[j] = sdf_.subtract(sdf_.shift(1))\n",
    "    surptime = data_df.combine_first(pd.DataFrame(surptime))\n",
    "    # getting by columns = surpries\n",
    "    surpint = {}\n",
    "    for i in tri_samples:\n",
    "        st_ = surptime[surptime['tri']==i].reindex(columns=cols)\n",
    "        surpint[i] = st_.median()\n",
    "    return pd.DataFrame(surpint).T\n",
    "\n",
    "\n",
    "# creating a combined dataframe\n",
    "vol_df = data_df.combine_first(ret_scores)\n",
    "# add the day count\n",
    "vol_df['day_count'] = pd.Series(index=vol_df.index,data=1).cumsum()\n",
    "# we now compute the surprise indicators\n",
    "vol_surp_perc = {}\n",
    "vol_surp_int = {}\n",
    "\n",
    "for estim in vol_estim:\n",
    "    # getting the surprise count across thresholds \n",
    "    surp_count = pd.DataFrame({threshnames[th]:calc_th(ret_scores['score_{0}'.format(estim)],th,True) for th in thresholds})\n",
    "    s_df = data_df.combine_first(surp_count)\n",
    "    vol_surp_perc[estim] = pctg_surprises(s_df)\n",
    "    \n",
    "    # getting the surprise intervals\n",
    "    surp_intervals = pd.DataFrame({threshnames[th]:calc_th(ret_scores['score_{0}'.format(estim)],th,True) for th in thresholds})\n",
    "    s_df_ = data_df.combine_first(surp_intervals)\n",
    "    vol_surp_int[estim] = med_surp_intervals(s_df_)\n",
    "\n",
    "# creating vol surprises count dataframe\n",
    "vol_surp_perc = pd.concat(vol_surp_perc)\n",
    "vol_surp_pval = pd.concat({estim: pd.DataFrame({i:util.estimate_pvalue(vol_surp_perc.loc[estim].loc[i],vol_surp_perc.loc[estim]) for i in os_samples}).rename(columns=sample_names) for estim in vol_estim})\n",
    "\n",
    "# creating surprise intervals dataframe\n",
    "vol_surp_int = pd.concat(vol_surp_int)\n",
    "vol_surp_int_pval = pd.concat({estim: pd.DataFrame({i:util.estimate_pvalue(vol_surp_int.loc[estim].loc[i],vol_surp_int.loc[estim]) for i in os_samples}).rename(columns=sample_names) for estim in vol_estim})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting volatility forecasts and levels date\n",
    "surpestimate = 'HL1M'\n",
    "\n",
    "# plotting the tails of volatility changes\n",
    "fig_ = plt.figure(figsize=(12,8))\n",
    "ax_test = util.box_scatter(vol_surp_perc.loc['HL1M'],os_samples=os_samples, plot_cols=['s_nnn','s_nn','s_n','s_p','s_pp','s_ppp'],scatter_names=sample_names,legend_dict={'loc':2},xlabel='Surprise Score',ylabel='Proportion of suprises in sample')\n",
    "ax_test.legend(loc=1,)\n",
    "ax_test.set_title('Fig 8: Surprise Rate',fontdict={'size':20})\n",
    "ax_test.set_xticklabels(['-2.33$\\sigma$','-1.96$\\sigma$','-1.65$\\sigma$','1.65$\\sigma$','1.96$\\sigma$','2.33$\\sigma$',])\n",
    "fig_.savefig('charts\\p8_volsurp_pctg.png')\n",
    "\n",
    "# # plotting the central chart \n",
    "fig_ = plt.figure(figsize=(12,8))\n",
    "ax_test = util.box_scatter(vol_surp_int.loc['HL1M'],os_samples=os_samples,plot_cols=['s_nnn','s_nn','s_n','s_p','s_pp','s_ppp'],scatter_names=sample_names, legend_dict={'loc':1}, xlabel='Surprise Score',ylabel='Days',)\n",
    "\n",
    "ax_test.set_title('Fig 9: Surprise Intervals',fontdict={'size':20})\n",
    "ax_test.set_xticklabels(['-2.33$\\sigma$','-1.96$\\sigma$','-1.65$\\sigma$','1.65$\\sigma$','1.96$\\sigma$','2.33$\\sigma$'])\n",
    "fig_.savefig('charts\\p9_volsurp_intv.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the percentile values here for formatting\n",
    "pvals = pd.concat({'returns':sample_pval,'vols':vol_sample_pval,'volch':volch_sample_pval,'vol_surp_pctg':vol_surp_pval.loc['HL1M'],'vol_surp_int':vol_surp_int_pval.loc['HL1M']})\n",
    "pvals.to_excel('pvals_copy.xlsx')"
   ]
  },
  {
   "source": [
    "## Bootstrapping Analysis (Extra)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will run a bootstrap here but probably put the results in the apppendix as an extra\n",
    "# getting the bootstrap samples\n",
    "bsample_3Y = util.bootstrap_by_year(data_df,ndraws=1000,ksample=3,)\n",
    "# generating the bootstrap distribution\n",
    "bdist_3Y = util.get_bootstrap_stats(bsample_3Y)\n",
    "btstrp_pval = pd.DataFrame({i:util.estimate_pvalue(sample_data_3Y.loc[i],bdist_3Y) for i in os_samples}).rename(columns=sample_names)\n"
   ]
  }
 ]
}